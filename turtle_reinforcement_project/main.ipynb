{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"transition 저장\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.timing = 0 #0 ~ 150\n",
    "        self.swing = None #Boolean\n",
    "        self.dim = 6\n",
    "        self.hidden_dim = 32\n",
    "        self.out_dim = 16 #0~150가능 -> 각 10당 할당, 나머지 하나는 no swing\n",
    "        self.linear1 = nn.Linear(self.dim, self.hidden_dim)\n",
    "        self.linear2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.linear3 = nn.Linear(self.hidden_dim, self.out_dim)\n",
    "        #이후 각 구간으로 나눈 다음 약간의 random성을 넣으면 좋을 듯\n",
    "        #만약 0~15이면 0~10까지 할당 +-5값은 random하게 줌. 나머지 하나는 no swing\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "\n",
    "    #forward에서 Standard Normal Distribution이후 해당 값을 변환\n",
    "    def forward(self, x): #x에는 env, 현재 투수\n",
    "        x = self.gelu(self.linear1(x))\n",
    "        x = self.gelu(self.linear2(x))\n",
    "        x = self.gelu(self.linear3(x))\n",
    "\n",
    "        return x #output의 경우 최종으로 변환하여 -50~200으로 하는 것이 좋을 듯. \n",
    "    #Monte Carlo 방식으로 학습하면 좋을 듯?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self) -> None:\n",
    "        self.total_score = 0\n",
    "        self.strike = 0\n",
    "        self.ball = 0\n",
    "        self.out = 0\n",
    "        self.pitcher = None\n",
    "        self.curr_base = [0, 0, 0] #각 1,2,3 루\n",
    "    \n",
    "    def get_env(self):\n",
    "        return [self.strike,self.ball, self.out,\n",
    "                self.pitcher, self.curr_base[0], self.curr_base[1], self.curr_base[2]]\n",
    "    \n",
    "    def strike_plus(self):\n",
    "        if self.strike == 2:\n",
    "            if self.out == 2:\n",
    "                self.strike, self.ball, self.out, self.curr_base = 0, 0, 0, [0, 0, 0]\n",
    "            else:\n",
    "                self.out += 1\n",
    "        else:\n",
    "            self.strike += 1\n",
    "\n",
    "    def ball_plus(self):\n",
    "        if self.ball == 3:\n",
    "            if self.curr_base[0] == 0: #1루가 비어있는 경우\n",
    "                self.curr_base[0] = 1\n",
    "            elif self.curr_base[0] == 1 and self.curr_base[1] == 1: #1, 2루가 채워 있는 경우\n",
    "                self.curr_base[2] = 1\n",
    "            elif self.curr_base[0] == 1 and self.curr_base[1] == 1 and self.curr_base[2] == 1:\n",
    "                self.total_score += 1\n",
    "            elif self.curr_base[0] == 1 and self.curr_base[2] == 1:\n",
    "                self.curr_base[1] = 1\n",
    "            elif self.curr_base[0] == 1:\n",
    "                self.curr_base[1] = 1\n",
    "        else:\n",
    "            self.ball += 1\n",
    "\n",
    "    def out_plus(self):\n",
    "        if self.out == 2:\n",
    "            self.strike, self.ball, self.out, self.curr_base = 0, 0, 0, [0, 0, 0]\n",
    "        else:\n",
    "            self.out += 1\n",
    "\n",
    "    def single(self):\n",
    "        if self.curr_base[2] == 1:\n",
    "            self.total_score += 1\n",
    "            self.curr_base[2] = 0\n",
    "        \n",
    "        if self.curr_base[1] == 1:\n",
    "            self.curr_base[2] = 1\n",
    "            self.curr_base[1] = 0\n",
    "        \n",
    "        if self.curr_base[0] == 1:\n",
    "            self.curr_base[1] = 1\n",
    "            #self.curr_base[0] = 1 --> 할 필요없음, 어차피 안타친 타자가 1루에 들어감\n",
    "\n",
    "    def double(self):\n",
    "        if self.curr_base[2] == 1:\n",
    "            self.total_score += 1\n",
    "            self.curr_base[2] = 0\n",
    "        \n",
    "        if self.curr_base[1] == 1:\n",
    "            self.total_score += 1\n",
    "            self.curr_base[1] = 0\n",
    "        \n",
    "        if self.curr_base[0] == 1:\n",
    "            self.curr_base[2] = 1\n",
    "            self.curr_base[1] = 1\n",
    "            self.curr_base[0] = 0\n",
    "\n",
    "    def triple(self):\n",
    "        if self.curr_base[2] == 1:\n",
    "            self.total_score += 1\n",
    "            self.curr_base[2] = 0\n",
    "        \n",
    "        if self.curr_base[1] == 1:\n",
    "            self.total_score += 1\n",
    "            self.curr_base[1] = 0\n",
    "        \n",
    "        if self.curr_base[0] == 1:\n",
    "            self.total_score += 1\n",
    "            self.curr_base[0] = 0\n",
    "            self.curr_base[2] = 1\n",
    "\n",
    "    def homerun(self):\n",
    "        if self.curr_base[2] == 1:\n",
    "            self.total_score += 1\n",
    "            self.curr_base[2] = 0\n",
    "        \n",
    "        if self.curr_base[1] == 1:\n",
    "            self.total_score += 1\n",
    "            self.curr_base[1] = 0\n",
    "        \n",
    "        if self.curr_base[0] == 1:\n",
    "            self.total_score += 1\n",
    "            self.curr_base[0] = 0\n",
    "\n",
    "        self.total_score += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pitcher():\n",
    "    def __init__(self, conditional) -> None:\n",
    "        self.fast_conditional = conditional\n",
    "        #예시 : [50.7, 41.8, 36.1, 67.4, 50.2]\n",
    "\n",
    "    def return_trajectory(self, env : Environment): #여기서 상황에 따라 자세히 구현해야 함.\n",
    "        random_number = random.random()\n",
    "\n",
    "        if env.strike == 0 and env.ball == 0:\n",
    "            if self.fast_conditional[0] < random_number:\n",
    "                return \"fastball\"\n",
    "            else:\n",
    "                return \"curve\"\n",
    "        \n",
    "        if env.strike > env.ball:\n",
    "            if self.fast_conditional[2] < random_number:\n",
    "                return \"fastball\"\n",
    "            else:\n",
    "                return \"curve\"\n",
    "            \n",
    "        if env.strike < env.ball:\n",
    "            if self.fast_conditional[3] < random_number:\n",
    "                return \"fastball\"\n",
    "            else:\n",
    "                return \"curve\"\n",
    "            \n",
    "        if env.strike == env.ball:\n",
    "            if self.fast_conditional[4] < random_number:\n",
    "                return \"fastball\"\n",
    "            else:\n",
    "                return \"curve\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE는 리플레이 버퍼에서 샘플링된 트랜지션의 수입니다.\n",
    "# GAMMA는 이전 섹션에서 언급한 할인 계수입니다.\n",
    "# EPS_START는 엡실론의 시작 값입니다.\n",
    "# EPS_END는 엡실론의 최종 값입니다.\n",
    "# EPS_DECAY는 엡실론의 지수 감쇠(exponential decay) 속도 제어하며, 높을수록 감쇠 속도가 느립니다.\n",
    "# TAU는 목표 네트워크의 업데이트 속도입니다.\n",
    "# LR은 ``AdamW`` 옵티마이저의 학습율(learning rate)입니다.\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN()\n",
    "target_net = DQN()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), amsgrad=True)\n",
    "memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max (1)은 각 행의 가장 큰 열 값을 반환합니다.\n",
    "            # 최대 결과의 두번째 열은 최대 요소의 주소값이므로,\n",
    "            # 기대 보상이 더 큰 행동을 선택할 수 있습니다.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randint(0, 16)]], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # 도표가 업데이트되도록 잠시 멈춤\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_step(action, env : Environment):\n",
    "    if action[0][0] == 16:\n",
    "        timing = -1 #no swing\n",
    "    else:\n",
    "        timing = (action[0][0] * 10 + 5) + 5 * random.uniform(-1, 1)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). 이것은 batch-array의 Transitions을 Transition의 batch-arrays로\n",
    "    # 전환합니다.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # 최종이 아닌 상태의 마스크를 계산하고 배치 요소를 연결합니다\n",
    "    # (최종 상태는 시뮬레이션이 종료 된 이후의 상태)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Q(s_t, a) 계산 - 모델이 Q(s_t)를 계산하고, 취한 행동의 열을 선택합니다.\n",
    "    # 이들은 policy_net에 따라 각 배치 상태에 대해 선택된 행동입니다.\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # 모든 다음 상태를 위한 V(s_{t+1}) 계산\n",
    "    # non_final_next_states의 행동들에 대한 기대값은 \"이전\" target_net을 기반으로 계산됩니다.\n",
    "    # max(1)[0]으로 최고의 보상을 선택하십시오.\n",
    "    # 이것은 마스크를 기반으로 병합되어 기대 상태 값을 갖거나 상태가 최종인 경우 0을 갖습니다.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    # 기대 Q 값 계산\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Huber 손실 계산\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # 모델 최적화\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # 변화도 클리핑 바꿔치기\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # 환경과 상태 초기화\n",
    "    state, info = Environment()\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward])\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # 메모리에 변이 저장\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # 다음 상태로 이동\n",
    "        state = next_state\n",
    "\n",
    "        # (정책 네트워크에서) 최적화 한단계 수행\n",
    "        optimize_model()\n",
    "\n",
    "        # 목표 네트워크의 가중치를 소프트 업데이트\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
